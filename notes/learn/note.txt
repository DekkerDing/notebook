名词
    Broker(代理)
        消息中间件处理节点一个Kafka节点就是一个broker
        一个或者多个Broker可以组成一个Kafka集群
    Topic(主题)
        Kafka根据topic对消息进行归类
        发布到Kafka集群的每条消息都需要指定一个topic
    Partition(分区)
        物理上的概念
        一个topic可以分为多个partition每个partition内部是有序的
    Replica(副本)
        副本每个partion有多个副本存储在不同的broker上
        保证消息的高可用
    Segment(片段)
        partition物理上由多个segment组成每个Segment存着message信息
    Message(消息)
        消息是基本的通讯单位
        由一个key
        一个value和时间戳构成
    Producer(生产者)
        消息生产者向Broker发送消息的客户端
    Consumer(消费者)
        消息消费者从Broker读取消息的客户端
    ConsumerGroup(消费者群组）
        每个Consumer属于一个特定的ConsumerGroup
        一条消息可以发送到多个不同的 ConsumerGroup
        但是一个ConsumerGroup中只能有一个Consumer能够消费该消息
    AR
        kafka中所有副本统称AR（AssignedRepllicas）
        AR=ISR+OSR
    ISR
        一组与Topic分区的Leader保持同步的Follower 分区副本
        ISR列表中的副本会定期向Leader 同步数据，确保数据的一致性
        和leader保持同步的follower集合
        如果follower超过replica.lag.time.max.ms默认30s没有同步 则会被踢出isr
    OSR
        表示follower与leader副本同步时，延迟过多的副本
    LEO
        每个副本的最后一个offset
        LEO（lastendoffset）就是该副本底层日志文件上的数据的最大偏移量的下一个值
        当我知道了LEO为10 我就知道该日志文件已经保存了10条信息，位移范围为[0,9]
    HW
        所有副本中最小的LEO
        指的是消费者能见到的最大的offset，ISR队列中最小的LEO。
生产者
    发送并忘记(fire-and-forget)
        producer.send(record);
    同步发送
        producer.send(record).get();
    异步发送
        producer.send(record, new Callback(){});
        Kafka的Producer发送消息采用的是异步发送的方式。
        在消息发送的过程中，涉及到了
        两个线程
        main线程和Sender线程，以及一个线程共享变量
        RecordAccumulator.
        main线程 将消息发送给 RecordAccumulator
        Sender线程 不断从 RecordAccumulator 中拉取消息发送到 Kafka broker。

    拦截器
        org.apache.kafka.clients.producer.Producerlnterceptor
        Interceptor可能被运行在多个线程中 因此在具体实现时用户需要自行确保线程安全
        指定了多个lnterceptor 则Producer将按照指定顺序调用它们 并仅仅是捕获每个lnterceptor可能抛出的异常记录到错误日志中而非在向上传递
    序列化器
        key
            key序列化器 org.apache.kafka.common.serialization.Serializer
        value org.apache.kafka.common.serialization.Serializer
            value序列化器
    分区器
        如果在记录中指定了分区，则使用指定的分区
        如果没有指定分区，但是有key的值，则使用key值的散列值计算分区
        如果没有指定分区也没有key的值，则使用轮询的方式选择一个分区
        org.apache.kafka.clients.producer.Partitioner
    缓冲区

    流程
        主线程 -> 生产的消息 -> [ 拦截器 序列化器 分区器 ] -> 消息累加器 -> Sender线程 -> 创建Request -> 封装并缓存为 (node信息+要发送的消息数据请求信息) -> 基于 NIO Selector发生
        生产者消息记录 -> send() 函数 -> [ 拦截器 序列化器 分区器 ] -> 缓冲区 -> 达到batch.size 或到达linger.ms -> Brokers(基于分区策略到达分区) -> 分区 -> 给生产者 ACK 响应 -> 重试 -> 缓冲区

    ack
        0
            生产者不确认消息是否发送成功 只要把他放入socket缓冲区就认为消息已经发送
        1
            生产者发送消息后 只要到了leader就认为消息已经发送成功
        -1
            生产者发送消息后 不仅到了leader 并且 leader还同步给了其他follower才算是同步
            isr
                isr指的是与leader数据同步的follower分区
            osr
                osr指的是与leader数据不那么同步 或者说复制进度落下的follower分区
    Exactly Once
        At Least Once
            将服务器的ACK级别设置为-1，可以保证Producer到Server之间不会丢失数据 即 At Least Once
        At Most Once
            相对的，将服务器ACK级别设置为O，可以保证生产者每条消息只会被发送一次，即 At Most Once
        AtLeast Once + 幂等性 = ExactlyOnce
            0.11版本的Kafka，引入了一项重大特性：幂等性。
            所谓的幂等性就是指Producer不论向Server发送多少次重复数据，Server端都只会持久化一条。
            幂等性结合AtLeastOnce语义就构成了Kafka的ExactlyOnce语义
            要启用幂等性，只需要将Producer的参数中 enable.idompotence 设置为true即可。
            Kafka 的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。
            开启幂等性的Producer在初始化的时候会被分配一个PID，发往同一Partition的消息会附带SequenceNumber。而
            Broker端会对<PID,Partition,SeqNumber>做缓存，当具有相同主键的消息提交时，Broker只会持久化一条。
            但是PID重启就会变化，同时不同的Partition也具有不同主键，所以幂等性无法保证跨分区跨会话的ExactlyOnce。
    事务
        Kafka从0.11版本开始引入了事务支持。
        事务可以保证Kafka在ExactlyOnce语义的基础上，生产和消费可以跨分区和会话
        要么全部成功，要么全部失败。
        Producer事务
            为了实现跨分区跨会话的事务，需要引入一个全局唯一的TransactionID，并将Producer
            获得的PID和TransactionID绑定。
            这样当Producer重启后就可以通过正在进行的Transaction ID 获得原来的PID。
            为了管理Transaction，Kafka引入了一个新的组件TransactionCoordinator。
            Producer就是通过和TransactionCoordinator交互获得TransactionID对应的任务状态。
            Transaction Coordinator还负责将事务所有写入Kafka的一个内部Topic
            这样即使整个服务重启，由于事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。
        Consumer 事务
            上述事务机制主要是从Producer方面考虑，对于Consumer而言，事务的保证就会相对较弱，尤其时无法保证Commit的信息被精确消费。
            这是由于Consumer可以通过offset访问任意信息，而且不同的 SegmentFile生命周期不同
            同一事务的消息可能会出现重启后被删除的情况。

    消息收集器
        消息收集器RecoderAccumulator为每个分区都维护了一个 Deque<ProducerBatch>类型的双端队列
        ProducerBatch
            ProducerBatch可以理解为ProducerRecord的集合
        由于生产者客户端使用java.io.ByteBuffer在发送消息之前进行消息保存，并维护了一个BufferPool实现ByteBuffer的复用；
        该缓存池只针对特定大小（batch.size指定）的ByteBuffer进行管理，如果消息过大的缓存，不能做到重复利用。
        每进入一个ProducerRecord都会寻找自己的ProducerBatch
        如果能写入则写入已自己的ProducerBatch
        如果不能写入那就新建一个ProducerBatch
    compression
        开启压缩compression.type=snappy
        压缩格式
            none
            gzip
            snappy
            lz4

    retries 该值为一个大于1的值指的是发送消息失败后重新发送消息的次数但重试可能导致消息乱序
        retry.backoff.ms 每次重试之间等待的时间

    request.timeout.ms 客户端等待响应的最大时长 如果超时 就会重新发送，除非达到重试次数
    该设置应该比replica.lag.time.max.ms大 以免服务器延迟时间内重试

    linger
        默认情况下 linger.ms=0ms 即缓冲区buffer.memory里面一有数据就会立刻被传输到broker里面去
        适当将linger.ms=5~100ms调大则在linger.ms时间内缓冲区里面会有更多数据
        此时再从缓冲区中拉取数据
        每次拉取的batch.size可以填充更多的数据

    batch.size 批量发送的数据量
        需要配合 linger 每次从缓冲里面拉取可能不足batch.size=16k

    send.buffer.bytes 利用TCP发送数据的时候使用的缓冲区

    connections.max.idle.ms 当连接空闲时间达到这个值 就关闭连接

    buffer.memory
        配置buffer.memory缓冲区，默认大小32M，33554432

    如何重试仍然保证消息不乱序？
        max.in.flight.requests.per.connection为1
        在确认一批消息发送成功并且确认后 才会发送下一批消息 这样就可以保证顺序 但是严重影响吞吐量
    零拷贝
        传统拷贝
            从磁盘中读取目标文件内容拷贝到内核缓中区
            CPU空制器再把内核缓中区的数据赋值到用户空间的缓中区中
            接着在应用程序中：调用write方法
            把用户空间缓中区中的数据考贝到内核下的socket Buffer中。
            把在内核模式下的 SocketBuffer中的数据赋值到网卡缓中区
            网卡缓中区再把数据传输到目标服务器上
                2次可优化拷贝
                    从内核空间赋值到用户空间
                    从用户空间再次赋值到内核空间
                    造成上下文切换
        零拷贝
            把这两次多于的拷贝省略掉
            应用程序可以直接把
            磁盘中的数据从内核直接传送到Socket
            零拷贝通过DMA技术 把文件内容 复制到内核空间的ReadBuffer
            接着把包含数据位置和长度信息的文件描述符加载到Socket Buffer中
            DMA引擎直接把数据从内核空间传递给网卡设备
            减少 CPU的上下文切换
            在Java中, FileChannal.transferTo 方法的底层实现就是sendfile方法
        mmap文件映射机制
            原理：将磁盘文件映射到内存，用户通过修改内存就能修改磁盘文件。
            使用这种方式可以获取很大的/O提升
            省去了用户空间到内核空间复制的开销
    优化
        batch.size 64KB-1MB
            批次大小指的是生产者在将消息发送到KafkaBroker之前会将多条消息收集到一个批次中进行发送。
            增大批次大小可以显著减少网络请求次数，因为每次发送更多的消息，从而提高了网络传输的效率，有助于提升整体的吞吐量。
            然而，这也会带来一定的延迟，因为生产者需要等待更多的消息填满批次。
            如果批次一直无法填满，消息就会在生产者端停留更长时间，直到达到其他触发发送的条件。
            在高吞吐量且对延迟要求不是特别苛刻的场景下，可以适当增大该值。
        linger.ms 10 - 100ms
        max.in.flight.requests.per.connection
            该参数限制了单个连接上允许的最大未确认请求数。
            在Kafka中，生产者可以在一个连接上同时发送多个请求而无需等待每个请求的响应。
            如果将该值设置得过高，可能会导致消息的顺序被破坏。
            例如，当一个请求失败并重试时，后续已经发送的请求可能会先到达Broker，从而导致消息的顺序不一致。
            因此，需要根据实际情况合理设置该值，以平衡吞吐量和消息顺序性。
    粘性分区
        指定消息 KEY 为  NULL
            原理：在这种策略下，Kafka会尽可能地将消息发送到同一个分区，直到这个分区的大小达到一定的阈值或者经过一定的时间间隔等条件触发后才会切换到下一个分区发送消息。
            这就好像消息“粘”在了某个分区上一样。
            提高性能：减少了频繁在不同分区之间切换的开销。
            因为向同一个分区发送消息可以利用缓存等机制，减少网络I/〇 和磁盘I/O 的次数。
            例如，在批量发送消息时，如果消息都发送到同一个分区，就可以利用操作系统和Kafka本身的缓存来提高写入效率。
            不能保证消息顺序 分区切换策略可能复杂
            对消息顺序要求不高，追求高性能的场景：例如在一些实时数据收集场景中，像传感器数据采集，只要能尽快地将数据发送到Kafka，数据的顺序不是关键因素，就可以采用这种策略来提高发送性能。
            大规模数据写入场景：如大数据日志收集平台，需要快速地将大量日志写入Kafka，使用粘性分区可以在保证一定负载均衡的情况下，提高写入效率。
消费者
    反序列化
        org.apache.kafka.common.serialization.Deserializer<T>接口
    拦截器
        org.apache.kafka.clients.consumer.Consumerlnterceptor<K, V>接口
    消费组
        多个从同一个主题拉取消息的消费者可以被归于一个消费者组中
        消费组均衡地给消费者分配分区 每个分区只由消费组中一个消费者消费

        单一主题下分区与消费组的三种对应关系
            分区数量多于消费者

            分区数量等于消费者

            分区数量小于消费者

        auto.offset.reset
            earliest：自动重置偏移量到最早的偏移量
            latest：自动重置偏移量为最新的偏移量
            none：如果消费组原来的（previous) 偏移量不存在，则向消费者抛异常
            anything：向消费者抛异常

        enable.auto.commit
            如果设置为true消费者会自动周期性地向服务器提交偏移量
            重复消费问题
                Consumer每5s提交offset 自动提交逻辑 如 Offset没有提交则发生 重复消费问题
                假设提交offset后的3s发生了Rebalance
                Rebalance之后的所有Consumer从上一次提交的offset处继续消费
                因此Rebalance发生前3s的消息会被重复消费
            手动提交
                同步提交
                    同步提交只有在commitSync方法返回最新offset的时候才会认为成功 会影响TPS 方法会被阻塞
                异步提交
                    commitAsync出现问题不会自动重试 (需要手动处理)
        fetch.min.bytes 每次拉取消息的请求返回数据量的最小值
        fetch.max.wait.ms 如果数据量达不到拉取最小值的话 最多间隔如此长时间进行数据拉取
    负载均衡
        再平衡规定了一个消费组的所有消费者如何来分配订阅主题的每个分区
        rebalance 什么时候再均衡
            组成员发生变更 (新消费者加入消费组 有消费者主动离开消费组 有消费者下线离开消费组)
            消费者组的协调器节点发生了变更
            topic的分区数量的新增或者减少
            订阅主题数发生变更
            订阅主题的分区数发生变更
        再平衡
            再平衡规定了同一消费组所有消费者如何分配topic中的每一个分区
            再平衡时消费者无法消费消息：直到再平衡结束为止
                消费组内消费者数量发生变更
                主体内的分区数量发生变更
                订阅的主题发生变化
            重平衡过程中 Rebalance
                消费者无法从kafka消费消息
                这对kafka的TPS影响极大而如果kafka集内节点较多
                    比如数百个那重平衡可能会耗时极多数分钟到数小时都有可能
                    而这段时间kafka基本处于不可用状态
                    所以在实际环境中应该尽量避免重平衡发生
            避免
                不可能完全避免再平衡
                因为消费者岩机
                分区增加
                消费者增加
                主题增加
                要么是我们主动添加要么无法避免，因此我们要尽力避免kafka认为消费者故障这种情况的发生

                session.timout.ms控制消费者心跳超时时间
                    session.timout.ms：设置为6s
                heartbeat.interval.ms控制消费者心跳发送频率
                    heartbeat.interval.ms： 设置2s
                max.poll.interval.ms控制poll的间隔
                    max.poll.interval.ms：推荐为消费者处理消息最长耗时再加1分钟
            命令
                Heartbeat请求：consumer需要定期给组协调器发送心跳来表明自已还活着
                LeaveGroup请求：主动告诉组协调器我要离开消费组
                SyncGroup请求：消费组Leader把分配方案告诉组内所有成员
                    一旦完成分配 Leader会将这个方案封装进SyncGroup请求中发给消费组协调器
                    非Leader也会发SyncGroup请求
                    只是内容为空
                    消费组协调器接收到分配方案之后会把方案塞进SyncGroup的response中发给各个消费者
                JoinGroup请求：成员请求加入组
                    所有成员都向消费组协调器发送JoinGroup请求
                    请求加入消费组
                    一旦所有成员都发送了JoinGroup请求协调器从中选择一个消费者担任Leader的角色
                    并把组成员信息以及订阅信息发给Leader
                DescribeGroup请求：显示组的所有信息，包括成员信息，协议名称，分配方案，订阅信息等
            一旦协调器发现某个消费者挂了怎么办？
                一旦协调器认为某个消费者挂了那么它就会开启新一轮再均衡
                并且在当前其他消费者的心跳响应中添加"REBALANCEINPROGRESS"
                告诉其他消费者：重新分配分区。
    pull
        pull 模式不足之处是，如果kafka 没有数据，消费者可能会陷入循环中，一直返回空数据。
        针对这一点，Kafka的消费者在消费数据时会传入一个时长参数timeout，如果当前没有数据可供消费，consumer会等待一段时间之后再返回，
        这段时长即为 timeout。
    poll
        poll：加入群组，接受分配到分区；轮询获取数据；发送心跳；自动提交
    close
        close：关闭网络连接；主动通知GroupCoordinator，自己已挂

    幂等性
    properties.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, "true");
        pid 每次重启kafka获得
        partition：分区号
        SeqNumber：自增号

    顺序消费
        kafka为什么会存在无序消费
            因为消费者是完全独立的一个网络节点
            所以可能会出现消息的消费顺序不是按照发送顺序来实现的
            从而导致消息的消费乱续的问题
            多分区生产消费速率不同导致 乱序
            单分区生产重试时导致 乱序
            当3失败后紧接看4成功了然后3 重试成功了这样数据就乱序了
            基于消息的 Key + Hash取模 放入指定分区如 KEY 是随机生成的没有规律就可能无法控制消费的有序性
        kafka如何保证有序消费
            指定消息的 KEY 让取模后的值落在固定分区中实现排队顺序
            自定义消息分区的一个路由算法
            批次排队实现顺序
                开启幂等性
                生产者在收到kafka响应之前可以投递多少个消息
                max.in.flight.requests.per.connection=1
            生产者在收到kafka响应之前可以投递多少个消息
            max.in.flight.requests.per.connection=5 retries>0允许重试
            会缓存5个数据，并保证5个数据内不会乱序(会在缓存过程中将消息进行排序)

    消息积压
        应用消费节点 < kafka partition 分区数，可以扩应用消费节点，否则，扩应用消费节点没用（应急：优先扩节点）
        判断生产分区数与消费者是否依依对应 如果 消费者小于分区数做消费者扩容
        自动提交改为手动提交
        单条消费消息改为批量消费消息，数据单条入库改为批量入库
        DB 入库考虑是否有事务流程性的数据保存尽量 拆开以免影响批量保存持久化时性能问题
        kafka partition 分区数 < 应用消费节点，可以扩broker分区数，否则，扩broker分区数没用（应急：优先扩节点）
        针对生产端采用动态配置开关降级，关闭MQ生产（系统不能快速扩容）
        如果是突发问题，临时扩容，增加消费者的数量。
        通过扩容和降级承担流量，应急问题的处理。
        其次，才是排查解决异常问题。
        监控、日志分析是否消费端的业务逻辑代码出现了问题，优化消费端的业务处理逻辑。
        最后，如果消费端处理不足，水平扩容提升消费端并发处理能力。
        在扩容消费者实例的同时，必须要同步扩容Topic分区的数量，确保消费者的实例数和分区数是相同的，分区是单线程消费。
        还有一种消息积压的情况是，日常系统正常运转的时候，没有积压或者只有少量积压很快就消费掉了，但是某一个时刻，突然就开始积压消息并且积压持续上涨。
        这种情况下需要你在短时间内找到消息积压的原因，迅速解决问题才不至于影响业务。
    延迟消息

    死信队列
        kafkaProducer.send(new ProducerRecord<>("dead_letter_topic"，log))；//写入死信队列

    事务消息
        生产者保证发送消息到 Serveer消息队列
            解决思想
            1. 如 网络通讯三次握手先发送一个网络确认消息 目的 保证生产者到消息队列网络通讯正常 (半消息)
            2. 消息队列响应这个网络确认消息到生产者服务中 可设计为 在发送消息体中带上 上一次生产者进程ID等唯一标识 做业务判断过滤处理
            3. 执行本地事务 基于 本地事务执行的成功或失败捕获异常 像中间协调者 发送 commit/rollback 中间协调者基于信号量策略判断是否发送至下一环节 消费者
            4. 中间协调者 当生产者迟迟未发送 执行本地事务结果时 可向生产者创建主动回查重试请求
    消息队列：目前RocketMQ中支持事务消息，它的工作原理是：
        a.生产者订单系统先发送一条half消息到Broker，half消息对消费者而言是不可见的
        b.再创建订单，根据创建订单成功与否，向Broker发送commit或rollback
        c.并且生产者订单系统还可以提供Broker回调接口，
        当Broker发现一段时间half消息没有收到任何操作命令，
        则会主动调此接口来查询订单是否创建成功
        d.一旦half消息commit了，消费者库存系统就会来消费，如果消费成功，则消息销毁，分布式事务成功结束
        e.如果消费失败，则根据重试策略进行重试，最后还失败则进入死信队列，等待进一步处理


    优化
        fetch.min.bytes 1M - 10MB

        fetch.max.wait.ms 500ms

        max.poll.records 500-5000

        auto.offset.reset earliest

Broker

    优化
        log.flush.interval.messages 1（立即刷盘，性能差)， n（批量刷盘，性能好，会丢数据）
        log.flush.interval.ms 1000-5000ms


消息的 推模式与拉模式
    push
        其实是基于消费端的长连接
        推模式可能在消费者不可达时
        大量消息积压再生产者端造成压力
    pull
        是消费者主动向Broker请求消息
        调整拉取间隔和批量大小
        这样可以减少空轮询带来的CPU浪费

Kappa架构
    你的应用程序实例中可能会有一个由Kafka更新的消息驱动的内存缓存。
    一种简单的实现方式是让Kafka主题的日志进行压缩并且在每次重启应用程序时从偏移量零开始重新加载以填充其缓存。
    流处理任务会在通过Kafka传输的数据流上执行计算。
    当流处理代码的逻辑改变时，你通常会希望重新计算结果。
    一个简单的方法就是将程序的偏移量重置为零，以新代码重新计算结果。
    这有时被称为Kappa架构（The Kappa Architecture）。

    Kafka常用来捕获并分发数据库更新的流（这通常被称为变更数据捕获或CDC）。
    消费这些数据的应用程序在稳定状态下只需要最新的更改，然而新的应用需要从完整的一次性导出或快照开始。
    然而，对大型生产数据库进行完整的导出往往是一个非常复杂且耗时的操作。
    对包含变更流的主题启用日志压缩，可以让消费者通过重置到偏移量零来简单地重新加载数据。
    所以，这样做是否疯狂？
    答案是否定的，在Kafka中存储数据没有任何疯狂之处：因为它是为此而设计的。
    Kafka中的数据被持久化到磁盘，经过校验，并且为了容错而进行了复制。
    存储更多数据并不会使它变慢。
    有一些生产环境中的Kafka集群已经存储了超过一拍字节的数据。
    那么，为什么将数据存储在Kafka中会引起人们的担忧，尽管它显然像一个存储系统？
    我认为人们的自然疑虑来自于Kafka经常被描述为消息队列。
    传统消息队列的头两条规则之一就是“你不应该在消息队列中存储消息”。
    在传统的消息系统中，这样做有很多原因行不通：
        因为读取消息的同时也移除了它；
        因为随着积累的数据超出内存容量，消息系统的表现会变差；
        因为消息系统通常缺乏强大的复制特性（如果代理挂掉，你的数据也可能丢失）
    这些都是传统消息系统设计中的重大缺陷。
    毕竟，想想看，任何异步消息本质上都是关于存储消息的，哪怕只是存储几秒钟直到它们被消费。
    毕竟，如果我的服务向某个队列发送一条消息并且继续其业务，但我希望保证你的服务最终能接收到并且处理这条消息，那么某处必须存储这条消息直到你的服务能够处理它。
    因此，如果你的消息系统不擅长存储消息，那它也不擅长“排队”消息。
    你可能会认为这无关紧要，因为你计划不会存储消息很长时间。
    但不管消息在消息系统中存储的时间多么短暂，如果该系统处于持续负载下，总会有一些未消费的消息被存储。
    所以每当它失败时，如果没有能力提供容错存储，就会丢失一些数据。
    正确地处理消息需要正确地处理存储。
    这一点看起来显而易见，但在评估消息系统时却常常被忽略。
    因此，对于消息使用案例来说，存储是很重要的。
    但实际上，Kafka并不是传统意义上的消息队列一一在实现上，
    它看起来与 RabbitMQ或其他类似技术完全不同。
    它的架构更接近于分布式文件系统或数据库而不是传统消息队列。Kafka与传统消息系统

    Kafka存储了一个可以重读并无限期保持的持久日志。
    Kafka作为一个现代分布式系统构建：它作为集群运行，可以弹性扩展或收缩，并且内部复制数据以确保容错性和高可用性。
    Kafka构建用于允许实时流处理，而不仅仅是逐条处理单一消息。这使得可以在更高层次的抽象级别上处理数据流。

    我们认为这些差异足以让我们认为将Kafka视为消息队列是不准确的，而是将其归类为流平台。

    事件重放
        如 当前 CPU 内存资源紧张 没太多的空余资源 支持如数据同步 数据加工后同步 但低峰谷时可将相关数据以重放形式拿取出来生成新消息供下游
        再 处理或存储 摇身一变成为流处理平台中间者

    思考一下消息系统存储系统和Kafka之间的关系
        可以得出以下结论。
        消息系统都是关于传播未来消息：当你连接到一个消息系统时，你在等待新消息的到来。
        文件系统或数据库这样的存储系统都是关于存储过去的写入：
            当你查询或从中读取时，你会得到基于过去更新的结果。
            流处理的核心在于能够结合这两者一一能够从过去处理
            并随着未来的到来继续处理