索引：
    创建索引
        PUT /es_index
    查询索引
        GET /es_index
        返回响应：
            别名 “aliases”： {}
            映射 “mappings”： {}
            settings {} 属性:
                索引名称 "provided_name": ""
                分片 "number_of_shards": ""
                    将一个物理大索引拆分成若干个小索引每一个小索引称为一个Shard
                    Index是逻辑概念，底层是若干个Shards 分布在若干节点上
                    个Shard最多可存放20亿个文档
                    路由算法（索引和查询）
                    shard number = hash(document id) % number_of_primary_shards
                    查看:
                        GET /_cat/indices?v
                    设置参考
                        视情况定：节点数量 每个节点的容量 索引数量/大小 查询模式
                        业界推荐单个shard<=50GB
                        Github实践128个shards，每个120GB
                        作者推荐25G～40GB（考虑堆内存需求）
                        500GB数据->10~20个Shards
                        推荐每GB堆内存支持最多20个Shards
                        设置jvm.options（Xms/Xmx），ES缺省1GB
                        3 Shards+3Replicas/Shard，每个Shard50GB
                        Shards存储需求3x50GB=150GB
                        Replicas存储需求3x3x50GB=450GB
                        日志类应用 单个分片不要大于50GB
                        搜索类应用 单个分片不要超过20GB
                    按需将索引的分片分配到特定的节点：


                副本 "numberofreplicas": ""
                    副本是分片的完全拷贝 也称replica shards
                    被复制的源Shard称为PrimaryShard
                    Primary Shard + Replica Shards统称Replication Group
                    查看
                        GET / cat/shards?v
                        GET / cluster/health
                    设置参考
                        副本是主分片的拷贝
                        副本会降低数据的索引速度：有几份副本就会有几倍的CPU资源消耗在索引I上
                        会减缓对主分片的查询压力 但是会消耗同样的内存资源
                        如果机器资源充分 提高副本数 可以提高整体的查询QPS
                        关键业务：
                            2+
                        非关键业务：
                            1
                        auto_expand_replicas
                    快照：
                        快照用于对某个索引或者对整个集群进行备份
                uuid "uuid": ""
                版本 "version": {}

                creation_date: "时间戳"
                分词器：
                    "index"：{
                        "analysis.analyzer.default.type"： "ik_max_word"
                    }
                    内置：
                        Standard Analyzer 一默认分词器 按词切分 小写处理
                        Simple Analyzer －按照非字母切分（符号被过滤） 小写处理
                        Stop Analyzer －小写处理 停用词过滤 (the, a, is)
                        Whitespace Analyzer 一按照空格切分 不转小写
                        Keyword Analyzer －不分词 直接将输入当作输出
                        Patter Analyzer 一正则表达式 默认\W+(非字符分隔）
                        Customer Analyzer 自定义分词器
    索引刷新 refresh
        refresh=true（或refresh）立即refresh文档可立即查询到
        refresh=wait_for 阻塞client直到设置的refresh-interval

        PUT es_index/_doc/1?refresh
        PUT es_index/_doc/1?refresh=true
        PUT es_index/_doc/1?refresh=false
        PUT es_index/_doc/1?refresh=wait_for

        PUT /es_index/_settings
        {
            "index": {
                "refresh_interval": "15s"
            }
        }

    Shard allocation awareness:

    Force awareness：

    SegmentFiles的数量：数量愈多对查询的速度与磁碟的空间愈不好
    Shard的数量：愈多对Indexing的速度愈好 但对查成本较高 单一Shard愈大对Cluster 的 Rebalance 的成本愈高
    Index的大小：愈大对于查询效率愈好 以Time-based资料来看的话 影响的是资料移转到下一个段的等待时间
    资料的新旧程度：新的资料通常使用频率较频繁 会给较好的硬体资源 较旧的资料较少使用 可配置较差的硬件资源
    时间粒度：当资料量很大时 在观察过往的资料往往时间粒度会抓较大 且观察的结果（例如：每天的log数量 每天的总销售金额 每天每部影片各别的观看次数...等）
    Index愈来愈多：资源总是有限，太旧的资料会面删除，可保留囊的结果
    资料的安全性：除了存取控制要妥善的限制之外 资料的备份也是非常重要的机制

    re-indexing 索引重建
        索引的Mappings发生变更：字段类型更改 分词器及字典更新
        索引的Settings发生变更：索引的主分片数发生改变
        集群内 集群间需要做数据迁移
        Update By Query: 在现有索引上重建
        Reindex：在其他索引上重建索引
    查看 Node 详情
        GET _cat/nodes
    索引是否存在
        HEAD /es_index
    删除索引
        DELETE /es_index
    关闭索引
        POST /es_index/close
    打开索引
        POST /es_index/_open
    重建索引
        POST _reindex
    查看索引
        GET /es_index/_settings
    显示索引状态为 green 的索引
        GET /_cat/indices?v&health=green
    按照文档个数排序
        GET /_cat/indices?v&s=docs.count:desc
    查看每个索引占用的内存信息
        GET /_cat/indices?v&h=i,tm&s=tm:desc

文档：
    元数据:
        "_index" 文档所属的索引名
        "_type" 文档所属的类型名
        "_id" 文档唯一ID
        "_source" 文档的原始JSON数据
        "_version" 文档的版本信息
        "_score" 相关性打分
    添加文档：
        指定 ID：
            PUT /es_index/_doc/1
        自生成ID：
            POST /es_index/_doc
        共同点： 底层是先做删除后更新 形成一种覆盖的形式
        基于文档ID判断后添加 (保证文档唯一性)：
            PUT /es_index/create/1
            POST /es_index/create/1
        批量添加：
            创建 底层要基于文档ID校验： (如果文档存在则抛出异常)
                POST _bulk
                {
                    "create": {
                        "_index": "xxx对应索引",
                        "_type": "_doc",
                        "_id": xxx
                    }
                }
                {
                    "id":xxx,
                    添加数据....
                }
            创建 如文档存在覆盖原来文档：
                POST _bulk
                {
                    "index": {
                        "_index": "xxx对应索引",
                        "_type": "_doc",
                        "_id": xxx
                    }
                }
                {
                    "id":xxx,
                    添加数据....
                }

    更新文档：

        指定 ID：
            POST /es_index/_update/1
            {
                "doc": {
                    更新文档的数据....
                },
                "doc_as_upsert": true # 如果该文档不存在则创建反之更新文档
            }

        基于查询条件更新： (类似于 Mysql Update xxx where xxx=xx )
            1 存储生成快照 -> 2 Search Query Replication Group -> 3 Bulk Update
            基于乐观锁概念 比较并判断 version_conflicts 字段 (是否有版本冲突)
            POST /es_index/_update_by_query
            {
                "query": {
                    "match":{
                        "_id": xx
                    }
                },
                "script": {
                    "source": "ctx._source.age = 30"
                },
                "conflicts": "proceed" # 当遇到快照冲突后续如何操作 proceed 继续执行  中止执行
            }

        upsert 更新:
            POST /es_index/_update/1
            {
                "upsert": {
                    更新数据....
                }
            }



    查询文档：
            Response 响应：
                took： 花费时间
                total: 符合条件的总文档数
                hits： 结果集
            全文挡查询：
                GET /es_index/_search
            基于ID查询：
                GET /es_index/_doc/1

            match (全文检索)：
                要 分词 算分
                条件查询 or
                    GET /es_index/_search
                    {
                        "query":{
                            "match": {
                                "xxx字段": "xxx"
                            }
                        }
                    }

                条件查询 and
                    GET /es_index/_search
                    {
                        "query":{
                            "match": {
                                "xxx字段": {
                                    "query":  "xxx字段值",
                                    "operator": "and"
                                }
                            }
                        }
                    }

                查询条件 至少匹配(几个)
                    GET /es_index/_search
                    {
                        "query":{
                            "match": {
                                "xxx字段": {
                                    "query": "xxx字段值",
                                    "minimum_should_match": 2 至少匹配几个
                                }
                            }
                        }
                    }

                match_phrase 短语匹配 (对比 AND 更加精确 要求查询的分词与结果是连续的)：
                    GET /es_index/_search
                    {
                        "query":{
                            "match_phrase": {
                                "xxx字段": {
                                    "query": "xxx字段值" # 要求查询的分词与结果是连续的 (查询条件是什么查询结果就是什么)
                                    "slop": 2 # 指定中间可以隔多少个字符
                                }
                            }
                        }
                    }

                多条件字段查询:
                    GET /es_index/_search
                    {
                        "query":{
                            "multi_match": {
                                "query": "xxx字段值",
                                "fields": [
                                    "xxx条件字段",
                                    "xxx条件字段"
                                    ...
                                ]
                            }
                        }
                    }

                指定拼接查询条件查询 query_string:
                                允许我们在单个查询字符串中指定AND丨OR丨NOT条件同时也和multi_matchquery一样支持多字段搜索。
                                和match类似但是 match需要指定字段名query_string是在所有字段中搜索范围更广泛。
                                注意：查询字段分词就将查询条件分词查询查询字段不分词将查询条件不分词查询

                                指定单个查询字段范围：
                                    POST /es_index/_search
                                    {
                                        "query":{
                                            "query_string": {
                                                "default_field": "xxx字段" #指定字段查询范围 只搜索这个字段匹配的
                                                "query": "xxx字段值 AND xxx字段值"
                                            }
                                        }
                                    }
                指定多个查询字段范围：
                    POST /es_index/_search
                    {
                        "query":{
                            "query_string": {
                                "fields": [
                                "xxx字段", #指定字段查询范围 只搜索这个字段匹配的
                                "xxx字段"
                                ...
                                ]
                                "query": "xxx字段值 AND xxx字段值"
                            }
                        }
                    }

                简化 指定多个查询字段范围：
                    POST /es_index/_search
                    {
                        "query":{
                            "simple_query_string": {
                                "fields": [
                                "xxx字段", #指定字段查询范围 只搜索这个字段匹配的
                                "xxx字段"
                                ...
                                ]
                                "query": "xxx字段值",
                                default_operator: "AND" # 比较符号
                            }
                        }
                    }

                指定返回值:
                    GET /es_index/_search
                    {
                        "query":{
                            "match_all": {},
                            "_source": [
                                "字段名称",
                                ...
                            ]
                        }
                    }
                    只看 source 字段里内容：
                        GET /es_index/_source/1
                    只看 元数据 内容:
                        GET /es_index/_doc/1?_source=false

            term：
            不分词 不算分
                精确查询 (如不进行分词)：
                    GET /es_index/_search
                    {
                        "query":{
                            "term": {
                                "FIELD": {
                                    "value": "xxx"
                                }
                            }
                        }
                    }
                精确查询 term
                查询字段映射类型为keyword
                    基于Query：
                        GET /es_index/_search
                        {
                            "query":{
                                "term": {
                                    "xxx字段.keyword": {
                                        "value": "xxx字段值"
                                    }
                                }
                            }
                        }
                    term 多值匹配
                        GET /es_index/_search
                        {
                            "query":{
                                "terms": {
                                    "xxx字段.keyword": [
                                        "xxx字段值"
                                            ...
                                        ]
                                    }
                                }
                            }
                        }

                    range 范围查找
                        range检索是Elasticsearch中一种针对指定字段值在给定范围内的文档的检索类型
                        这种查询适合对数字日期或其他可排序数据类型的字段进行范围筛选
                        range检索支持多种比较操作符，如大于(gt)、大于等于(gte)、小于(It)和小于等于(lte)等，可以实现灵活的区间查询

                        Elasticsearch支持日期数学表达式，允许在查询和聚合中使用相对时间点。以下是一些常见的日期数学表达式的示例和解释
                        now：当前时间点
                        now-1d：从当前时间点向前推1天的时间点
                        now-1w：从当前时间点向前推1周的时间点
                        now-1M：从当前时间点向前推1个月的时间点
                        now-1y：从当前时间点向前推1年的时间点
                        now+1h：从当前时间点向后推1小时的时间点

                        GET /es_index/_search
                        {
                            "query":{
                                "range": {
                                    "xxx字段": {
                                        "get": 25, # 大于
                                        "lte": 25, # 小于
                                        "lt": 25,
                                        "gt": 25,
                                        }
                                    }
                                }
                            }
                        }

                    exists 检查特定字段是否存在
                        GET /es_index/_search
                        {
                            "query":{
                                "exists": {
                                    "field": "xxx字段"
                                    }
                                }
                            }
                        }

                    基于多个ID查询
                        GET /es_index/_search
                        {
                            "query":{
                                "ids": {
                                    "values": [
                                            id,
                                            ...
                                        ]
                                    }
                                }
                            }
                        }

                    基于 constant_score：
                        通过constantscore转成filtering 没有算分
                        GET /es_index/_search
                        {
                            "query":{
                                "constant_score": {
                                    "filter": {
                                        "term": {
                                            "xxx字段.keyword": "xxx字段值"
                                        }
                                    }
                                }
                            }
                        }
                前缀搜索 prefix:
                prefix会对分词后的term进行前缀搜索
                它不会对要搜索的字符串分词传入的前缀就是想要查找的前缀
                默认状态下前缀查询不做相关性分数计算它只是将所有匹配的文档返回然后赋予所有相关分数值为1
                它不会分析要搜索字符串传入的前缀就是想要查找的前缀
                默认状态下前缀查询不做相关度分数计算它只是将所有匹配的文档返回然后赋予所有相关分数值为1。
                它的行为更像是一个过滤器而不是查询。
                两者实际的区别就是过滤器是可以被缓存的而前缀查询不行。
                需要遍历所有倒排索引并比较每个词项是否以所搜索的前缀开头
                    GET /es_index/_search
                    {
                        "query":{
                            "prefix": {
                                "xxx字段.keyword": {
                                    "value": "xxx字段值"
                                }
                            }
                        }
                    }

                通配符查询 wildcard
                通配符查询可能会导致较高的计算负担因此在实际应用中应谨慎使用尤其是在涉及大量文档的情况下
                    GET /es_index/_search
                    {
                        "query":{
                            "wildcard": {
                                "xxx字段": {
                                    "value": "*xxx字段值*" # 如 *白*
                                }
                            }
                        }
                    }

                错误模糊冗余搜索 fuzzy
                    GET /es_index/_search -XP0ST http://localhost:9200/_bulk --data-binary "@bulk_data.json.txt"
                    {
                        "query":{
                            "fuzzy": {
                                "xxx字段": {
                                    "value": "*xxx字段值"
                                    "fuzziness"： 2 # 冗余错误数量 AUTO
                                }
                            }
                        }
                    }

                terms_set 多值基于条件个数精确匹配
                    GET /es_index/_search
                    {
                        "query":{
                            "terms_set": {
                                "xxx字段": {
                                    "terms": [
                                        xxx字段值,
                                        ...
                                    ],
                                    "minimum_should_match_script"： {
                                        "source": "2" # 条件个数
                                    }
                                }
                            }
                        }
                    }

            批量查询：
            curl -H "Content-Type: application/x-ndjson"
                基于ID与不同索引查询：
                    GET _mget
                    {
                        "docs":[
                            {
                                "_index": "索引名称",
                                "_id": xxx
                            }
                            ...
                        ]
                    }

                基于多条件与不同索引查询：
                    GET /_msearch
                    {
                        "index": "xxx"
                    }
                    {
                        "query": {
                            "match_all": {}
                        },
                        "size": xxx,
                        "from": 0
                    }

            布尔查询 bool Query:
                一个bool查询是一个或者多个查询子句的组合总共包括4种子句其中2种会影响算分2种不影响算分
                must:相当于&&必须匹配贡献算分 must:相当于&& 必须匹配 贡献算分 (内容合并)
                    可包含多个查询条件每个条件均满足的文档才能被搜索到每次查询需要计算相关度得分属于搜索上下文
                should:相当于|丨选择性匹配贡献算分 should:相当于! 选择性匹配 贡献算分 (内容合并)
                    可包含多个查询条件不存在must和fiter条件时至少要满足多个查询条件中的一个文档才能被搜索到否则
                    需满足的条件数量不受限制，匹配到的查询越多相关度越高，也属于搜索上下文
                must_not:相当于！必须不能匹配不贡献算分 must_not:相当于！ 必须不能匹配 不贡献算分
                    可包含多个过滤条件每个条件均满足的文档才能被搜索到每个过滤条件不计算相关度得分结果在一定条件下会被缓存属于过滤上下文
                filter:必须匹配不贡献算法 filter：必须匹配 不贡献算法
                    可包含多个过滤条件每个条件均不满足的文档才能被搜索到每个过滤条件不计算相关度得分结果在一定条件下会被缓存属于过滤上下文
                QueryContext:相关性算分
                FilterContext:不需要算分（Yes|No）,可以利用Cache获得更好的性能
                相关性并不只是全文本检索的专利也适用于yes丨no的子句匹配的子句越多相关性评分越高
                如果多条查询子句被合并为一条复合查询语句比如bool查询则每个查询子句计算得出的评分会被合并到总的相关性评分中

                布尔查询可以按照布尔逻辑条件组织多条查询语句只有符合整个布尔条件的文档才会被搜索出来

                搜索上下文(querycontext)：使用搜索上下文时Elasticsearch需要计算每个文档与搜索条件的相关度得分这个得分的计算
                需使用一套复杂的计算公式，有一定的性能开销，带文本分析的全文检索的查询语句很适合放在搜索上下文中

                过滤上下文（filtercontext)：使用过滤上下文时，Elasticsearch只需要判断搜索条件跟文档数据是否匹配，例如使用Term query判断一个值是否跟搜索内容一致
                使用Range query判断某数据是否位于某个区间等。
                过滤上下文的查询不需要进行相关度得分计算还可以使用缓存加快响应速度很多术语级查询语句都适合放在过滤上下文中
                自定义控制查询权重
                    negative_boost
                    negative_Boost对negative部分query生效
                    计算评分时,boosting部分评分不修改 negative部分query乘以negative_boost值
                    negative_boost取值:0-1.0,举例:0.3
                    GET /es_index/_search
                    {
                        "query": {
                            "boosting": {
                                # 关心的查询
                                "positive": {
                                    "match": {
                                        "要查询的字段名"： "查询的值"
                                    }
                                }
                            },
                            "negative"： {
                                # 不关心的查询
                                "match": {
                                    "要查询的字段名"： "查询的值"
                                }
                            },
                            "negative_boost": 权重值
                        }
                    }

                    boost：

            分页查询:
                10000以内:
                    GET /es_index/_search
                    {
                        "query":{
                            "match_all": {},
                            "size": xxx,
                            "from": 0
                        }
                    }
                设置修改条数限制：
                    PUT /es_index/_settings
                    {
                        "index.max_result_window":"20000"
                    }
                快照 Scroll：
                    查询命令中新增scro11=1m说明采用游标查询保持游标查询窗口一分钟。
                    这里由于测试数据量不够所以size值设置为2
                    实际使用中为了减少游标查询的次数可以将值适当增大比如设置为1000
                    GET /es_index/_search?scroll=1m
                    {
                        "query": {
                            "match_all": {}
                        },
                        "size": xxx
                    }

                    scroll_id的值就是上一个请求中返回的_scroll_id的值
                    GET /_search/scroll
                    {
                        "scroll": "1m",
                        "scroll_id": "xxxx"
                    }
                search_after
                第一步搜索需要指定 sort 并且保证值是唯一的(可以通过加入 id 保证唯一性)
                然后使用上一次 最后一个文档的 sort值进行查询
                GET /es_index/_search
                {
                    "size": 1,
                    "query": {
                        "match_all": {}
                    },
                    "search_after":[13,"上一次随机ID"],
                    "sort": [
                        {"_id","asc"}
                    ]

                }
                排序：
                Elasticsearch默认会以文档的相关度算分进行排序
                当指定了排序字段 Elasticsearch 就不会进行算分
                    GET /es_index/_search
                    {
                        "query":{
                            "match_all": {},
                            "sort": [
                                {
                                    "字段名称": "desc"
                                }
                            ]
                        }
                    }

    分词器：
        POST _analyze
        {
            "analyzer": "ik_max_word",
            "text": "值"
        }

        GET /_analyze
        {
            "analyzer": "分词器",
            "text": "要分词的文本"
        }

        POST _analyze
        {
            "field": "指定的字段",
            "analyzer": "ik_max_word",
            "text": "值"
        }
    删除文档：
        基于ID删除:
            DELETE /es_index/_doc/1
        基于查询删除:
            POST es_index/_delete_by_query
            {
                "query": {
                    查询匹配条件
                }
            }
    跨集群搜索:


别名:
    场景:
        跨多个索引数据统计
    POST /_aliases
    {
        "actions": [
            {
                "add": {
                    "index": "es_index",
                    "alias": "别名名称"
                }
            }
            ...
        ]
    }

相关性
    TF-IDF：
        TF是词频(Term Frequency)
            检索词在文档中出现的频率越高 相关性也越高。
        IDF是逆向文本频率(InverseDocumentFrequency)
            每个检索词在索引中出现的频率 频率越高 相关性越低。
        字段长度归一值（field-lengthnorm）
            字段的长度是多少？字段越短 字段的权重越高。
            检索词出现在一个内容短的title要比同样的词出现在一个内容长的 content 字段权重更大。I
            以上三个因素
                词频（termfrequency）
                逆向文档频率（inversedocumentfrequency）
                字段长度归一值（field-length norm）
                是在索引l时计算并存储的最后将它们结合在一起计算单个词在特定文档中的权重。
    BM25

索引模板：
    使用 索引创建设置组件化 基于索引模板前缀匹配,实现复用通用 索引 的 settings mappings
    索引模版只适用于新创建的索引

    静态模板
        template_
    动态模板
        dynamic_templates


Mapping 映射
    查看:
        GET /es_index/_mapping
    显示设置
        PUT es_index
        {
            "mappings": {
                "properties": {
                    "xxx字段"： {
                        "type": "constant_keyword",
                        "value": "xxx字段值"
                    }
                }
            }
        }
        PUT es_index
        {
            "mappings": {
                "properties": {
                    "xxx字段"： {
                        "type": "字段类型"
                        "index": false # Index－控制当前字段是否被索引 默认为true 如果设置成 false 该字段不可被搜索
                    }
                }
            }
        }
        index_options:
            四种不同级别的IndexOptions配置 可以控制倒排索引l记录的内容 记录内容越多 占用存储空间越大
            docs － 记录 doc id
            freqs － 记录 doc id 和 term frequencies
            positions - 记录 doc id / term frequencies / term position
            offsets - doc id / term frequencies / term posistion / character offects
            Text 类型默认记录postions，其他默认为docs
        需要对Null值实现搜索:
            只有Keyword类型支持设定Null_Value
            PUT es_index
            {
                "mappings": {
                    "properties": {
                        "xxx字段"： {
                            "type": "Keyword"
                            "null_value" "NULL"
                        }
                    }
                }
            }
        copy_to:
            copy_to将字段的数值拷贝到目标字段
            PUT es_index
            {
                "mappings": {
                    "properties": {
                        "firstName"： {
                            "type": "text"
                            "copy_to" "fullName"
                        },
                        "lastName"： {
                            "type": "text"
                            "copy_to" "fullName"
                        }
                    }
                }
            }
            效果：
                GET users/_search?q=fullName:(Ruan Yiming)

    类型
        boolean
        float 小数点
        date "format" : "yyyy/MM/dd HH:mm:ss yyyy/MM/dd epoch_millis"
        long 表示数值
        integer 表示数值
        double 表示浮点数
        binary
        object
        nested 表示一个对象数组
        Keyword 对值的准确匹配主要用于准确匹配/过滤/聚合/排序
        用于id 枚举及不需要分词的文本 例如电话号码 email地址 手机号码 邮政编码 性别等
        适用于 Filter (精确匹配） Sorting 和 Aggregations
            constant_keyword
            wildcard
        text 表示文本信息 字符串值 无结构数据
        用于全文本字段 文本会被Analyzer分词
        默认不支持聚合分析及排序 需要设置fielddata为true
        默认会为文本类型设置成text 并且设置一个keyword的子字段
        如不需要检索 排序和聚合分析 Enable设置成false
        如不需要排序或者聚合分析功能
        Doc_values/fielddata设置成false
        如不需要检索 Index设置成false
        对需要检索的字段 可以通过如下配置 设定存储粒度
        Index_options / Norms ： 不需要归一化数据时 可以关闭
        更新频繁 聚合查询频繁的keyword类型的字段
        推荐将 eager alobal ordinals设置为 true
            search_as_you_type
            match_only_text
            completion
            token_count
            etc
        geo_point
        geo_shape
        percolator

聚合分析
    Bucket Aggregation 列满足特定条件的文档的集合
        类似于 GROUP by
    {
        "aggs": {
            "by_size":{
                "terms": {
                    "field": "要分组的字段"
                }
            }

        }
    }

    Metric Aggregation 数学运算，可以对文档字段进行统计分析
        类属于 COUNT() 等SQL统计函数
        SELECT MIN(price) MAX(price） FROM products
    {
        "aggs": {
            "自定义名称"： {
                "avg": {
                    "field"： "字段名称"
                }
            }
        }
    }

    Pipeline Aggregation 对其他的聚合结果进行二次聚合
    Matrix Aggregration 支持对多个字段的操作并提供一个结果矩阵
    {
        "size":0,
        "aggs": {
            "自定义别名"：{
                "max": {
                    "field": "字段名称"
                }
                ...
            }
        }
    }
    sub-aggregation



parent
child

优化
    写入：
        提高文档写入吞吐量
        过程：
            Refresh
                将文档先保存在 Index buffer 中
                以 refresh_interval 为间隔时间定期清空 buffer
                生成 segment, 借助文件系统缓存的特性
                先将segment 放在文件系统缓存中
                并开放查询 以提升搜索的实时性
            Translog
                Segment没有写入磁盘 即便发生了当机 重启后 数据也能恢复 默认配置是每次请求都会落盘
            Flush
                删除l旧的 translog 文件
                生成 Segment 并写入磁盘 /更新 commit point 并写入磁盘 ES 自动完成 可优化点不多
        客户端
            多线程
            批量写
        服务器
            降低IO操作
            使用ES自动生成的文档ld
            Refresh Interval
            降低 CPU和存储开销
            减少不必要分词
                只需要聚合不需要搜索 Index设置成false
                不需要算分 Norms 设置成 false
                不要对字符串使用默认的 dynamic mapping 字段数量过多 会对性能产生比较大的影响
                Index_options 控制在创建倒排索引|时 哪些内容会被添加到倒排索引中 优化这些设置 一定程度可以节约 CPU
                关闭_source 减少 IO 操作 （适合指标型数据）
            避免不需要的doc_values
            文档的字段尽量保证相同的顺序 可以提高文档的压缩率
            尽可能做到写入和分片的均衡负载 实现水平扩展
            Shard Filtering
            Write Load Balancer
            调整BuIk线程池和队列
                单个buk请求体的数据量不要太大 官方建议大约5-15mb
                写入端的bulk请求超时需要足够长 建议60s以上
                写入端尽量将数据轮询打到不同节点
                索引创建属于计算密集型任务
                应该使用固定大小的线程池来配置
                来不及处理的放入队列 线程数应该配置成 CPU 核心数 +1
                避免过多的上下文切换
            牺牲可靠性：将副本分片设置为0，写入完毕再调整回去
            牺牲搜索实时性：增加RefreshInterval的时间
            增大静态配置参数 indices.memory.index_buffer_size 默认是 10% 会导致自动触发 refresh
            Index.translog.durability：默认是request 每个请求都落盘 设置成 async 异步写入
            Index.translog.flush_threshod_size:默认 512 mb 可以适当调大 当 translog 超过该值 会触发 flush
            Index.translog.sync_interval 设置为 60s 每分钟执行一次
            牺牲搜索实时性：增加RefreshInterval的时间
            合理设置主分片数 确保均匀分配在所有数据节点上
            Index.routing.allocation.total_share_per_node:限定每个索引|在每个节点上可分配的主分片数
            5个节点的集群。索引有5个主分片，1个副本，应该如何设置?
            (5+5)/5=2
            生产环境中要适当调大这个数字 避免有节点下线时 分片无法正常迁移
            Index Alias 无需停机 无需修改程序 即可进行修改
Segment 段合并:
    优化点：降低最大分段大小 避免较大的分段继续参与Merge 节省系统资源 （最终会有多个分段）
    Index.merge.policy.segments_per_tier，默认为 10 越小需要越多的合并操作
    Index.merge.policy.max_merged_segment,默认5 GB 操作此大小以后，就不再参与后续的合并操作
Hot Warm Architecture
    数据通常不会有Update操作；适用于Timebased索引l数据（生命周期管理） 同时数据量比较大的场景。
    引入Warm节点，低配置大容量的机器存放老数据，以降低部署成本
    Hot 节点(通常使用 SSD）：索引I有不断有新文档写入。通常使用 SSD
    Warm 节点（通常使用 HDD）：索引|不存在新数据的写入；同时也不存在大量的数据查询
    Shard Filtering
        标记节点(Tagging)
            标记一个Hot节点
                bin/elasticsearch -E node.name=hotnode -E cluster.name=geektime -E path.data=hot_data -E node.attr.my_node_type=hot
            标记一个warm节点
                bin/elasticsearch -E node.name=warmnode -E cluster.name=geektime E path.data=warm_data -E node.attr.my_node_type=warm
            GET /_cat/nodeattrs?v
        配置索引到HotNode
            HotPhase：新的資料大量寫入HotNodes 要快 就配置较多的Primaryshards
            Rollover：随時間及資料量的成長 透過Rollover將Index進行rotate 產生新的Index來接新的資料 而原先的 Index會進入下一個Warm的段
        配置索引到Warm节点
            WarmPhase：進入到Warmdata 的段 會進入read-only 所以會透過ForceMerge與Shrink将Segment Files數量與Shards數量進行最佳化 也可同時配合Compress進行储存空間的優化
            ColdPhase：資料進入Codedata段 會將Index進行Freeze 以减少JVMheap的使用量 提供较高的延反應 但還是能即時使用的服務狀態
            Delete：再更久的資料，可再確已經被備份過之後進行刪除
        Rollup 可以 Hot,Warm,Cold 任何的Index当中把資料出 且以较大的 時間颗粒度進行總運算 將結果储存新的 Rollup Index中
        Rollup是定時執行 同時Rollup的資料在透過_rollup_search查询時 可混搭LiveData+RollupData 結果會自動合供去除掉重覆的 也因此當舊的資料被刪除後 存在Rollup的資料一樣能提供總後的查結果
        Cron Job + Batch => Aggregation (Date Histogram, Histogram, Terms, Metrics)
        Experimental 目前是立的功能，因此Rollup Index不能Rollover 未來會轉成為 ILM當中的一個Action
时间序列的索引
    按照时间进行划分索引 会使得管理更加简单。例如 完整删除一个索引 性能比 delete by query 好 定期关闭或者删除索引

与算分和分词相关的查询操作：
    match
        operator
        minimum_should_match_script
    match_phrase
    multi_match
    query_string
    simple_query_string

不使用算分和分词的查询操作：
结构化数据范围包括：
日期 布尔类型和数字都是结构化的
如彩色笔可以有离散的颜色集合：红(red)、绿(green、蓝(blue)
一个博客可能被标记了标签 例如，分布式(distributed)和搜索(search)
电商网站上的商品都有UPC（通用产品码UniversalProductCode）或其他的唯一
    term
        分词忽略大小写
        PUT es_index
        {
            "settings": {
                "analysis":{
                    "normalizer": {
                        "es_normalizer": {
                            "filer": [
                                "lowercase",
                                "asciifolding"
                            ],
                            "tyoe": "custom"
                        }
                    }
                }
            }
        }
    constant_score
    filter
        不会进行算分
        会使用查询缓存
    前缀查询 prefix
        默认状态下 前缀查询不做相关度分数计算
        它只是将所有匹配的文档返回 然后赋予所有相关分数值为1
        它的行为更像是一个过滤器而不是查询
        两者实际的区别就是过滤器是可以被缓存的 而前缀查询不行
        需要遍历所有倒排索引 并比较每个term是否已所指定的前缀开头 (有性能问题)
    通配符查询 wildcard
        工作原理和prefix相同 只不过它不是只比较开头，它能支持更为复杂的匹配模式
    范围查询 range
    日期 range date
    多id查询ids
    模糊查询 fuzzy

Transform
    將資料以Pivot的方式進行分析運算將結果存在獨立的Index中
    適合複雜的Aggregation的運算及報表的定期處理工作
    也可以這個機制查询的總結果建立成Index 當作Ingest处理時Enrich資料的LookupDataSource
     =>每月營銷報表、每月產品銷量分析、每日影片播放量

使用SnapshotLifecycleManagement（SLM）來管理資料的備份
    設定 定期 備份的保存時間及數量，確保備份所估用的空間不會無限增長
